{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmrWYq4FpRI1oLQ9hVoFNj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cmcnaidu123/akshayathaMaamCMC/blob/master/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k43EDkmmxRIh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import BiopythonTranslator, GraphicFeature, GraphicRecord\n",
        "import annotate_biopython_record"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "5lP2_YtPx3JV",
        "outputId": "3b8c48b5-f710-4bfc-fb99-af514a333064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-805bfe521d4b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mBiopythonTranslator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGraphicFeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGraphicRecord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mannotate_biopython_record\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'BiopythonTranslator'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "record = SeqIO.read(\"../../../databases/GCF_000195955.2_ASM19595v2_genomic.gbff\", \"genbank\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "teGqthWp0iwD",
        "outputId": "fb1fc831-fd2a-4935-bd19-0bbd2973322d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-dc6305f64fb8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrecord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeqIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../../databases/GCF_000195955.2_ASM19595v2_genomic.gbff\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"genbank\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'SeqIO' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "import h5py\n",
        "print(\"h5py version:\", h5py.__version__)\n",
        "import json\n",
        "import deeplift\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "IB7tZRTF2ooV",
        "outputId": "c418dfee-4677-4d78-94a9-e2f182d93ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h5py version: 3.9.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-658012b5a76d>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"h5py version:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdeeplift\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deeplift'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install deeplift"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Hi0LpP93A5L",
        "outputId": "fec8fae6-8980-44a1-c34f-7c2e464a8255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deeplift\n",
            "  Downloading deeplift-0.6.13.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.10/dist-packages (from deeplift) (1.23.5)\n",
            "Building wheels for collected packages: deeplift\n",
            "  Building wheel for deeplift (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeplift: filename=deeplift-0.6.13.0-py3-none-any.whl size=36424 sha256=50eb528e9afa3afe382eb5824a6ba6c92a2e31914f7c67fe99a283a09c818d32\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/d0/62/3f2d52f229601a9107ad95750d9b403c66063756100d374b6f\n",
            "Successfully built deeplift\n",
            "Installing collected packages: deeplift\n",
            "Successfully installed deeplift-0.6.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sparse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeObdXHy3Llv",
        "outputId": "519eb735-4371-4c4c-ac5a-9801186424d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sparse\n",
            "  Downloading sparse-0.14.0-py2.py3-none-any.whl (80 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.0/81.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sparse) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.10/dist-packages (from sparse) (1.11.2)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.10/dist-packages (from sparse) (0.56.4)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->sparse) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->sparse) (67.7.2)\n",
            "Installing collected packages: sparse\n",
            "Successfully installed sparse-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sparse\n",
        "import deeplift"
      ],
      "metadata": {
        "id": "-XgMGGcF3QSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkhY2WIR3tj8",
        "outputId": "019b2aa4-7329-47fb-c32a-6136b6479d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.13.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"Tensorflow version:\", tf.__version__)\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "print(\"Numpy version:\", np.__version__)\n",
        "import tensorflow.keras.backend as K\n",
        "import pandas as pd\n",
        "import deeplift.conversion.kerasapi_conversion as kc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KU0B1qJ3erg",
        "outputId": "25862f62-e7e3-485d-a12f-a4f0a622e702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version: 2.13.0\n",
            "Numpy version: 1.23.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Keras version:\", keras.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "uR4AcVpL383O",
        "outputId": "f368ac52-1f4f-40c2-df73-39247f629a3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-63c676fdde08>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Keras version:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'keras.api._v2.keras' has no attribute '__version__'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import models\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score"
      ],
      "metadata": {
        "id": "1W5sadSY4Lfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import model_from_json\n",
        "from deeplift.layers import NonlinearMxtsMode\n",
        "from collections import OrderedDict\n",
        "from deeplift.util import compile_func"
      ],
      "metadata": {
        "id": "QN-rEy1U4ZFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####### Section 1: Read in the reference input data and convert to one-hot #########"
      ],
      "metadata": {
        "id": "Pj2Gg70H4gxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_shapes(df_geno):\n",
        "\t\t\"\"\"\n",
        "\t\tFinds the length of each gene in the input dataframe\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tdf_geno_pheno: pd.Dataframe\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tdict of str: int\n",
        "\t\t\tlength of coordinates in each column\n",
        "\t\t\"\"\"\n",
        "\t\tshapes = {}\n",
        "\t\tfor column in df_geno.columns:\n",
        "\t\t\tif \"one_hot\" in column:\n",
        "\t\t\t\tshapes[column] = df_geno.loc[df_geno.index[0],column].shape[0]\n",
        "\n",
        "\t\treturn shapes"
      ],
      "metadata": {
        "id": "1y2-1veA4b5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h37rv_geno = pd.read_pickle(\"../model_training/h37rv_geno.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "9-SPolEF46jM",
        "outputId": "a4ee5985-2e12-4c52-d8b9-515952a4044a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-2e4ad85f5d83>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mh37rv_geno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../model_training/h37rv_geno.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \"\"\"\n\u001b[1;32m    189\u001b[0m     \u001b[0mexcs_to_catch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../model_training/h37rv_geno.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import glob\n",
        "import os\n",
        "import yaml\n",
        "import sparse\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tb_cnn_codebase import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "wyM61o6E58Rk",
        "outputId": "8acf5d58-5da4-4605-aee7-f755a890ff02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-c6e86c583f7b>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtb_cnn_codebase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tb_cnn_codebase'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_multi_weighted_bce(alpha, y_pred):\n",
        "\n",
        "\t\"\"\"\n",
        "\tCalculates the masked weighted binary cross-entropy in multi-classification\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\talpha: an element from the alpha matrix, a matrix of target y values weighted\n",
        "\t\tby proportion of strains with resistance data for any given drug\n",
        "\ty_pred: model-predicted y values\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tscalar value of the masked weighted BCE.\n",
        "\t\"\"\"\n",
        "\ty_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())\n",
        "\ty_true_ = K.cast(K.greater(alpha, 0.), K.floatx())\n",
        "\tmask = K.cast(K.not_equal(alpha, 0.), K.floatx())\n",
        "\tnum_not_missing = K.sum(mask, axis=-1)\n",
        "\talpha = K.abs(alpha)\n",
        "\tbce = - alpha * y_true_ * K.log(y_pred) - (1.0 - alpha) * (1.0 - y_true_) * K.log(1.0 - y_pred)\n",
        "\tmasked_bce = bce * mask\n",
        "\treturn K.sum(masked_bce, axis=-1) / num_not_missing\n",
        "\n",
        "def masked_weighted_accuracy(alpha, y_pred):\n",
        "\n",
        "\t\"\"\"\n",
        "\tCalculates the mased weighted accuracy of a model's predictions\n",
        "\tParameters\n",
        "\t----------\n",
        "\talpha: an element from the alpha matrix, a matrix of target y values weighted\n",
        "\t\tby proportion of strains with resistance data for any given drug\n",
        "\ty_pred: model-predicted y values\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tscalar value of the masked weighted accuracy.\n",
        "\t\"\"\"\n",
        "\n",
        "\ttotal = K.sum(K.cast(K.not_equal(alpha, 0.), K.floatx()))\n",
        "\ty_true_ = K.cast(K.greater(alpha, 0.), K.floatx())\n",
        "\tmask = K.cast(K.not_equal(alpha, 0.), K.floatx())\n",
        "\tcorrect = K.sum(K.cast(K.equal(y_true_, K.round(y_pred)), K.floatx()) * mask)\n",
        "\treturn correct / total\n",
        "\n",
        "def convertKerasJSONtoDeepLIFT(kerasJSON_str):\n",
        "    jsonData = json.loads(kerasJSON_str)\n",
        "    layersData = jsonData[\"config\"][\"layers\"]\n",
        "    jsonData[\"config\"] = layersData\n",
        "    return json.dumps(jsonData)\n"
      ],
      "metadata": {
        "id": "VyweiKzP8LtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filter_size = 13\n",
        "model = keras.models.Sequential()\n",
        "model.add(layers.Conv2D(\n",
        "    64, (4, filter_size),\n",
        "    data_format='channels_last',\n",
        "    activation='relu',\n",
        "    input_shape = (4, 10291, 18), name='conv2d'\n",
        "))"
      ],
      "metadata": {
        "id": "d91w4gM36wGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(layers.Conv2D(64, (1,12), activation='relu', name='conv10d'))\n",
        "model.add(layers.MaxPooling2D((1,3), name='max_pooling1d'))\n",
        "model.add(layers.Conv2D(32, (1,3), activation='relu', name='conv1d_1'))\n",
        "model.add(layers.Conv2D(32, (1,3), activation='relu', name='conv1d_2'))\n",
        "model.add(layers.MaxPooling2D((1,3), name='max_pooling1d_1'))\n",
        "model.add(layers.Flatten(name='flatten'))\n",
        "model.add(layers.Dense(256, activation='relu', name='d1'))\n",
        "model.add(layers.Dense(256, activation='relu', name='d2'))\n",
        "model.add(layers.Dense(13, activation='sigmoid', name='d4'))\n",
        "\n",
        "model.compile(optimizer=Adam(lr=np.exp(-1.0 * 9)),\n",
        "              loss=masked_multi_weighted_bce,\n",
        "              metrics=[masked_weighted_accuracy])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUi0IeJb7nxZ",
        "outputId": "f09e7715-9ce7-4219-e977-1f4a1c3d189f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 1, 10279, 64)      59968     \n",
            "                                                                 \n",
            " conv10d (Conv2D)            (None, 1, 10268, 64)      49216     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling2  (None, 1, 3422, 64)       0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv1d_1 (Conv2D)           (None, 1, 3420, 32)       6176      \n",
            "                                                                 \n",
            " conv1d_2 (Conv2D)           (None, 1, 3418, 32)       3104      \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPoolin  (None, 1, 1139, 32)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 36448)             0         \n",
            "                                                                 \n",
            " d1 (Dense)                  (None, 256)               9330944   \n",
            "                                                                 \n",
            " d2 (Dense)                  (None, 256)               65792     \n",
            "                                                                 \n",
            " d4 (Dense)                  (None, 13)                3341      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9518541 (36.31 MB)\n",
            "Trainable params: 9518541 (36.31 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import glob\n",
        "import os\n",
        "import yaml\n",
        "import sparse\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from keras.layers import *\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from Bio import SeqIO"
      ],
      "metadata": {
        "id": "x2qtYnUJ8m3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping to use for one-hot encoding\n",
        "BASE_TO_COLUMN = {'A': 0, 'C': 1, 'T': 2, 'G': 3, '-': 4}\n",
        "\n",
        "# Get one hot vector\n",
        "def get_one_hot(sequence):\n",
        "    \"\"\"\n",
        "\tCreates a one-hot encoding of a sequence\n",
        "\tParameters\n",
        "\t----------\n",
        "\tsequence: iterable of str\n",
        "\t\tSequence containing only ACTG- characters\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tnp.ndarray of int\n",
        "\t\tL (seq len) x 5 one-hot encoded sequence\n",
        "\t\"\"\"\n",
        "\n",
        "    seq_len = len(sequence)\n",
        "    seq_in_index = [BASE_TO_COLUMN.get(b, b) for b in sequence]\n",
        "    one_hot = np.zeros((seq_len, 5))\n",
        "\n",
        "    # Assign the found positions to 1\n",
        "    one_hot[np.arange(seq_len), np.array(seq_in_index)] = 1\n",
        "\n",
        "    return one_hot"
      ],
      "metadata": {
        "id": "8QWC7qDp-Foz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sequence_dictionary(filename):\n",
        "    \"\"\"\n",
        "\tCreates a dataframe that contains the sequence of each locus for each isolate\n",
        "\tNote that this function splits the identifier names in the fasta file on '/'\n",
        "\tand takes the last entry\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tfilename: str\n",
        "\t\tpath to directory containing genotype data (one fasta file containing\n",
        "\t\tsequences for all isolates at a particular locus)\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tpd.DataFrame with one column, indexed by strain name\n",
        "\t\tcolumn name will be the beginning string of the file name\n",
        "\t\"\"\"\n",
        "    seq_dict = SeqIO.to_dict(\n",
        "        SeqIO.parse(filename, \"fasta\"),\n",
        "        key_function=lambda x: x.id.split(\"/\")[-1].split(\".cut\")[0])\n",
        "\n",
        "    # create a dictionary of identifier: sequence\n",
        "    for identifier, sequence in seq_dict.items():\n",
        "        seq_dict[str(identifier)] = str(sequence.seq)\n",
        "\n",
        "    df = pd.DataFrame.from_dict(seq_dict, orient='index')\n",
        "    gene_name = filename.split(\"/\")[-1].split(\"_\")[0]\n",
        "    df.columns = [gene_name]\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "ODNpQaoI-LBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_genotype_df(genotype_input_directory):\n",
        "    \"\"\"\n",
        "    Create a dataframe with the genotypes for each isolate at each locus\n",
        "    Hard-codes the ordering of the loci so that they are in same order upon re-runs\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tgenotype_input_directory: str\n",
        "\t\tpath to directory containing fasta files of genotype inputs\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tpd.DataFrame:\n",
        "\t\tindexed by isolate name, one column per locus\n",
        "\t\"\"\"\n",
        "    # Make a df that combines all genotype data\n",
        "    dfs_list = []\n",
        "\n",
        "    locus_order = [\n",
        "        \"acpM-kasA\",\n",
        "        \"gid\",\n",
        "        \"rpsA\",\n",
        "        \"clpC\",\n",
        "        \"embCAB\",\n",
        "        \"aftB-ubiA\",\n",
        "        \"rrs-rrl\",\n",
        "        \"ethAR\",\n",
        "        \"oxyR-ahpC\",\n",
        "        \"tlyA\",\n",
        "        \"KatG\",\n",
        "        \"rpsL\",\n",
        "        \"rpoBC\",\n",
        "        \"FabG1-inhA\",\n",
        "        \"eis\",\n",
        "        \"gyrBA\",\n",
        "        \"panD\",\n",
        "        \"pncA\"\n",
        "    ]\n",
        "\n",
        "    for l in locus_order:\n",
        "        df_files = glob.glob(f\"{genotype_input_directory}/{l}*.fasta\")\n",
        "        if len(df_files)==1:\n",
        "            df_file = df_files[0]\n",
        "        else:\n",
        "            raise ValueError\n",
        "        print(\"reading fasta file\", df_file)\n",
        "        _df = sequence_dictionary(df_file)\n",
        "        print(\"found this many seqs\", len(_df))\n",
        "        dfs_list.append(_df)\n",
        "    df_genos = dfs_list[0].join(dfs_list[1:], how='outer')\n",
        "    print(\"size of df geno\", len(df_genos))\n",
        "    return df_genos"
      ],
      "metadata": {
        "id": "80GY5PaE-S75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rs_encoding_to_numeric(df_geno_pheno, drugs_list):\n",
        "    \"\"\"\n",
        "\tCreates a matrix of y values (resistance/sensitivity)\n",
        "\tto each drug, encoded as 0's and 1's\n",
        "\n",
        "    Note: encodes sensitivity as 1, resistance as 0, missing as -1\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tdf_geno_pheno: pd.DataFrame\n",
        "        Dataframe containing resistance values to be converted from \"R\" and \"S\" to 0 and 1\n",
        "\n",
        "    drugs_list: list of str\n",
        "        list of columns in df_geno_pheno containing the drug resistance info to be encoded\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tpd.Dataframe\n",
        "        has same index as df_geno_pheno, columns corresponding to drugs_list\n",
        "        contains numeric encoding of resistance values\n",
        "\n",
        "    np.ndarray\n",
        "        corresponds to above pd.DataFrame.values\n",
        "\t\"\"\"\n",
        "\n",
        "    y_all_rs = df_geno_pheno[drugs_list]\n",
        "    y_all_rs = y_all_rs.fillna('-1')\n",
        "    y_all_rs = y_all_rs.astype(str)\n",
        "    resistance_categories = {'R': 0, 'S': 1, '-1.0': -1, '-1': -1}\n",
        "\n",
        "    y_all = y_all_rs.copy()\n",
        "    for key, val in resistance_categories.items():\n",
        "        y_all[y_all_rs == key] = val\n",
        "\n",
        "    y_all.index = list(range(0, y_all.shape[0]))\n",
        "\n",
        "    y_all_array = y_all.values\n",
        "\n",
        "    return y_all, y_all_array\n",
        "\n",
        "\n",
        "def alpha_mat(subset_y, df_geno_pheno, weight=1.):\n",
        "    \"\"\"\n",
        "\tcreates alpha matrix (reflects proportion of strains resistant\n",
        "\t(-ve)/sensitive (+ve)).\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tdf_geno_pheno: pd.DataFrame\n",
        "\t\tDataframe where last 23 columns contain R/S encoding\n",
        "\t\tof resistance vs sensitivity to antibiotics\n",
        "\n",
        "\tsubset_y: np.ndarray\n",
        "\t\tDataframe generated by the function rs_encoding_to_numeric\n",
        "\t\tcontaining matrix of resistance values (0 or 1) for each drug,\n",
        "\t\tthen subset for indexes (strains) with phenotype data available\n",
        "    weight: float\n",
        "        Weight by which to multiply the sensitive class (to up or downweight\n",
        "        sensitive relative to resistant strains)\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tnp.ndarray of weighted resistance/sensitivity values proportionate to no.\n",
        "\t\tof strains with phenotypic data.\n",
        "\t\"\"\"\n",
        "    # Drugs\n",
        "    drugs = ['RIFAMPICIN', 'ISONIAZID', 'PYRAZINAMIDE',\n",
        "             'ETHAMBUTOL', 'STREPTOMYCIN', 'LEVOFLOXACIN',\n",
        "             'CAPREOMYCIN', 'AMIKACIN', 'MOXIFLOXACIN',\n",
        "             'OFLOXACIN', 'KANAMYCIN', 'ETHIONAMIDE',\n",
        "             'CIPROFLOXACIN']\n",
        "\n",
        "    num_drugs = len(drugs)\n",
        "\n",
        "    y_cnn = subset_y\n",
        "\n",
        "    # generate alpha matrix\n",
        "    alphas = np.zeros(num_drugs, dtype=np.float)\n",
        "    alpha_matrix = np.zeros_like(y_cnn, dtype=np.float)\n",
        "\n",
        "    for drug in range(num_drugs):\n",
        "\n",
        "        resistant = len(np.squeeze(np.where(y_cnn[:, drug] == 0.)))\n",
        "        sensitive = len(np.squeeze(np.where(y_cnn[:, drug] == 1.)))\n",
        "        alphas[drug] = resistant / float(resistant + sensitive)\n",
        "        alpha_matrix[:, drug][np.where(y_cnn[:, drug] == 1.0)] = weight * alphas[drug]\n",
        "        alpha_matrix[:, drug][np.where(y_cnn[:, drug] == 0.0)] = - alphas[drug]\n",
        "\n",
        "    return alpha_matrix\n",
        "\n",
        "\n",
        "def make_geno_pheno_pkl(**kwargs):\n",
        "    \"\"\"\n",
        "    Creates and saves a pd.DataFrame as a pkl that contains the numeric encoded\n",
        "    phenotype information and the one-hot encoded sequence information for each isolate\n",
        "\n",
        "    Required kwargs:\n",
        "        phenotype_file: path to input phenotype file with columns \"Isolate\" and drug names\n",
        "        genotype_input_directory: path to directory with input fasta files\n",
        "        pkl_file: path to save the complete genotype/phenotype file\n",
        "\t\"\"\"\n",
        "\n",
        "    output_path = kwargs['output_path']\n",
        "\n",
        "    # get table for phenotypes\n",
        "    df_phenos = pd.read_csv(kwargs['phenotype_file'], index_col=\"Isolate\", sep=\",\", dtype=str).fillna(-1)\n",
        "\n",
        "    # make table of all genotypes\n",
        "    df_genos = make_genotype_df(kwargs['genotype_input_directory'])\n",
        "\n",
        "    # to save on RAM, only save genotypes for isolates for which we have phenotypes\n",
        "    isolate_ids = list(df_phenos.index)\n",
        "    n_strains = len(isolate_ids)\n",
        "    df_genos.index = df_genos.index.astype(str)\n",
        "    df_genos = df_genos.loc[df_genos.index.intersection(isolate_ids)]\n",
        "\n",
        "    # Drop rows where we're missing the sequence for a locus\n",
        "    df_genos = df_genos.dropna(axis=\"index\")\n",
        "\n",
        "    #\n",
        "    # # Apply one-hot encoding function to get each isolate sequence\n",
        "    print('making one hot encoding for...')\n",
        "    for column in df_genos.columns:\n",
        "        print(\"...\", column)\n",
        "        df_genos[column + \"_one_hot\"] = df_genos[column].apply(np.vectorize(get_one_hot))\n",
        "\n",
        "    # combined dataframe of all genotypes and phenotypes\n",
        "    df_geno_pheno_full = df_genos.join(df_phenos, how='inner')\n",
        "\n",
        "    pkl_file = kwargs[\"pkl_file\"]\n",
        "    df_geno_pheno_full.to_pickle(pkl_file)\n",
        "\n",
        "\n",
        "def create_X(df_geno_pheno):\n",
        "    \"\"\"\n",
        "\tCreate an input X matrix, with output dimensions:\n",
        "\t\tn_strains x 5 (one-hot) x longest locus length x no. of loci\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_geno_pheno: pd.DataFrame\n",
        "        generated by make_geno_pheno_pkl, contains the numeric encoded\n",
        "        phenotype information and the one-hot encoded sequence information for each isolate\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        with shape N_strains, 5, L_longest_locus, N_loci\n",
        "        contains the one-hot encoded sequence information for each locus for each strain\n",
        "\t\"\"\"\n",
        "\n",
        "    def _get_shapes(df_geno_pheno):\n",
        "        \"\"\"\n",
        "\t\tFinds the length of each gene in the input dataframe\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tdf_geno_pheno: pd.Dataframe\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tdict of str: int\n",
        "\t\t\tlength of coordinates in each column\n",
        "\t\t\"\"\"\n",
        "        shapes = {}\n",
        "        for column in df_geno_pheno.columns:\n",
        "            if \"one_hot\" in column:\n",
        "                shapes[column] = df_geno_pheno.loc[df_geno_pheno.index[0], column].shape[0]\n",
        "\n",
        "        return shapes\n",
        "\n",
        "    shapes = _get_shapes(df_geno_pheno)\n",
        "\n",
        "    # Length of longest gene locus\n",
        "    n_genes = len(shapes)\n",
        "    L_longest = max(list(shapes.values()))\n",
        "    print(\"found n genes\", n_genes, \"and longest gene\", L_longest)\n",
        "\n",
        "    # Number of strains in model\n",
        "    n_strains = df_geno_pheno.shape[0]\n",
        "\n",
        "    # define shape of matrix - fill with zeros (effectively accomplishes padding)\n",
        "    X = np.zeros((n_strains, 5, L_longest, n_genes))\n",
        "\n",
        "    # for each strain and gene locus\n",
        "    for idx, strain in enumerate(df_geno_pheno.index):\n",
        "        for gene_index, gene in enumerate(shapes.keys()):\n",
        "            one_hot_gene = df_geno_pheno.loc[strain, gene]\n",
        "            X[idx, :, range(0, one_hot_gene.shape[0]), gene_index] = one_hot_gene\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "def masked_multi_weighted_bce(alpha, y_pred):\n",
        "    \"\"\"\n",
        "\tCalculates the masked weighted binary cross-entropy in multi-classification\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\talpha: an element from the alpha matrix, a matrix of target y values weighted\n",
        "\t\tby proportion of strains with resistance data for any given drug\n",
        "\ty_pred: model-predicted y values\n",
        "    weights: list of float (optional, default=[1., 1.])\n",
        "        A list of two weights to be applied to the sensitive and resistant n_strains,\n",
        "        respectively\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tscalar value of the masked weighted BCE.\n",
        "\t\"\"\"\n",
        "    y_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())\n",
        "    y_true_ = K.cast(K.greater(alpha, 0.), K.floatx())\n",
        "    mask = K.cast(K.not_equal(alpha, 0.), K.floatx())\n",
        "    num_not_missing = K.sum(mask, axis=-1)\n",
        "    alpha = K.abs(alpha)\n",
        "    bce = - alpha * y_true_ * K.log(y_pred) - (1.0 - alpha) * (1.0 - y_true_) * K.log(1.0 - y_pred)\n",
        "    masked_bce = bce * mask\n",
        "    return K.sum(masked_bce, axis=-1) / num_not_missing\n",
        "\n",
        "\n",
        "def masked_weighted_accuracy(alpha, y_pred):\n",
        "    \"\"\"\n",
        "\tCalculates the mased weighted accuracy of a model's predictions\n",
        "\tParameters\n",
        "\t----------\n",
        "\talpha: an element from the alpha matrix, a matrix of target y values weighted\n",
        "\t\tby proportion of strains with resistance data for any given drug\n",
        "\ty_pred: model-predicted y values\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tscalar value of the masked weighted accuracy.\n",
        "\t\"\"\"\n",
        "\n",
        "    total = K.sum(K.cast(K.not_equal(alpha, 0.), K.floatx()))\n",
        "    y_true_ = K.cast(K.greater(alpha, 0.), K.floatx())\n",
        "    mask = K.cast(K.not_equal(alpha, 0.), K.floatx())\n",
        "    correct = K.sum(K.cast(K.equal(y_true_, K.round(y_pred)), K.floatx()) * mask)\n",
        "    return correct / total\n",
        "\n",
        "def load_alpha_matrix(alpha_matrix_path, y_array, df_geno_pheno, **kwargs):\n",
        "    \"\"\"\n",
        "    Loads in the alpha matrix, if file exists, otherwise creates alpha matrix\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha_matrix_path: str\n",
        "        path to alpha matrix. Will be used to save matrix if matrix does not exist\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        The alpha matrix\n",
        "    \"\"\"\n",
        "\n",
        "    if os.path.isfile(alpha_matrix_path):\n",
        "        print(\"alpha matrix already exists, loading alpha matrix\")\n",
        "        alpha_matrix = alpha_matrix = np.loadtxt(alpha_matrix_path, delimiter=',')\n",
        "    else:\n",
        "        print(\"creating alpha matrix\")\n",
        "        if \"weight_of_sensitive_class\" in kwargs:\n",
        "            print('creating alpha matrix with weight', kwargs[\"weight_of_sensitive_class\"])\n",
        "            alpha_matrix = alpha_mat(y_array, df_geno_pheno, kwargs[\"weight_of_sensitive_class\"])\n",
        "        else:\n",
        "            alpha_matrix = alpha_mat(y_array, df_geno_pheno)\n",
        "        np.savetxt(alpha_matrix_path, alpha_matrix, delimiter=',')\n",
        "\n",
        "    print(\"the shape of the alpha_matrix: {}\".format(alpha_matrix.shape))\n",
        "    return alpha_matrix\n",
        "\n",
        "def split_into_traintest(X_sparse, df_geno_pheno, category):\n",
        "    \"\"\"\n",
        "    Splits the X dataframe into training and test set based on annotation in df_geno_pheno\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_sparse: sparse.COO\n",
        "        a sparse-encoded np.ndarray containing genetic information for all isolates\n",
        "    df_geno_pheno: pd.DataFrame\n",
        "        Dataframe of genetic and phenotypic information. Contains the exact isolates in the exact order used to create X_sparse.\n",
        "        Contains a column called \"category\" that will be used to split isolates into training and test\n",
        "    category: str\n",
        "        Name of the training set category\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    sparse.COO\n",
        "        a sparse-encoded np.ndarray containing genetic information for all TRAINING SET isolates\n",
        "    sparse.COO\n",
        "        a sparse-encoded np.ndarray containing genetic information for all TEST SET isolates\n",
        "    \"\"\"\n",
        "    X_all = X_sparse.todense()\n",
        "\n",
        "    df_geno_pheno = df_geno_pheno.reset_index(drop=True)\n",
        "\n",
        "    train_indices = df_geno_pheno.query(\"category==@category\").index\n",
        "    test_indices = df_geno_pheno.query(\"category!=@category\").index\n",
        "\n",
        "    print(\"splitting X pkl\")\n",
        "    X_train = X_all[train_indices, :]\n",
        "    X_test = X_all[test_indices, :]\n",
        "    del X_all\n",
        "\n",
        "    X_sparse_train = sparse.COO(X_train)\n",
        "    sparse.save_npz(pkl_file_sparse_train, X_sparse_train, compressed=False)\n",
        "\n",
        "    X_sparse_test = sparse.COO(X_test)\n",
        "    sparse.save_npz(pkl_file_sparse_test, X_sparse_test, compressed=False)\n",
        "\n",
        "    return X_sparse_train, X_sparse_test\n",
        "\n",
        "\n",
        "def get_threshold_val(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute the optimal threshold for prediction  based on the max sum of specificity and Sensitivity\n",
        "\n",
        "    NB that we encoded R as 0, S as 1, so smaller predictions indicate higher chance of resistance\n",
        "\n",
        "    We count falsely predicted resistance as a false positive, falsely predicted sensitivity as a false negative\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true: np.array\n",
        "        Actual labels for isolates\n",
        "    y_pred: np.array\n",
        "        Predicted labels for isolates\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict of str -> float with entries:\n",
        "        sens: sensitivity at chosen threshold\n",
        "        spec: specificity at chosen threshold\n",
        "        threshold: chosen threshold value\n",
        "    \"\"\"\n",
        "\n",
        "    num_samples = y_pred.shape[0]\n",
        "    fpr_ = []\n",
        "    tpr_ = []\n",
        "    thresholds = np.linspace(0, 1, 101)\n",
        "    num_sensitive = np.sum(y_true==1)\n",
        "    num_resistant = np.sum(y_true==0)\n",
        "    for threshold in thresholds:\n",
        "\n",
        "        fp_ = 0 # number of false positives\n",
        "        tp_ = 0 # number of true positives\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            # If y is predicted resistant\n",
        "            if (y_pred[i] < threshold):\n",
        "                if (y_true[i] == 1): fp_ += 1\n",
        "                if (y_true[i] == 0): tp_ += 1\n",
        "\n",
        "        fpr_.append(fp_ / float(num_sensitive))\n",
        "        tpr_.append(tp_ / float(num_resistant))\n",
        "\n",
        "    fpr_ = np.array(fpr_)\n",
        "    tpr_ = np.array(tpr_)\n",
        "\n",
        "    # valid_inds = np.where(fpr_ <= 0.1)\n",
        "    valid_inds = np.arange(101)\n",
        "    sens_spec_sum = (1 - fpr_) + tpr_\n",
        "    best_sens_spec_sum = np.max(sens_spec_sum[valid_inds])\n",
        "    best_inds = np.where(best_sens_spec_sum == sens_spec_sum[valid_inds])\n",
        "\n",
        "    if best_inds[0].shape[0] == 1:\n",
        "        best_sens_spec_ind = np.array(np.squeeze(best_inds))\n",
        "    else:\n",
        "        best_sens_spec_ind = np.array(np.squeeze(best_inds))[-1]\n",
        "\n",
        "    return {'threshold': np.squeeze(thresholds[valid_inds][best_sens_spec_ind]),\n",
        "            'spec': 1 - fpr_[valid_inds][best_sens_spec_ind],\n",
        "            'sens': tpr_[valid_inds][best_sens_spec_ind]}"
      ],
      "metadata": {
        "id": "5rQKKntL-eEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Functions for running CNN on MTB data to predict ABR phenotypes using tensorflow 1\n",
        "Authors:\n",
        "\tMichael Chen (original version)\n",
        "\tAnna G. Green\n",
        "\tChang Ho Yoon\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import sys\n",
        "import glob\n",
        "import os\n",
        "import yaml\n",
        "import sparse\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "from Bio import SeqIO\n",
        "\n",
        "# Mapping to use for one-hot encoding\n",
        "BASE_TO_COLUMN = {'A': 0, 'C': 1, 'T': 2, 'G': 3, '-': 4}\n",
        "\n",
        "# Get one hot vector\n",
        "def get_one_hot(sequence):\n",
        "    \"\"\"\n",
        "\tCreates a one-hot encoding of a sequence\n",
        "\tParameters\n",
        "\t----------\n",
        "\tsequence: iterable of str\n",
        "\t\tSequence containing only ACTG- characters\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tnp.ndarray of int\n",
        "\t\tL (seq len) x 5 one-hot encoded sequence\n",
        "\t\"\"\"\n",
        "\n",
        "    seq_len = len(sequence)\n",
        "    seq_in_index = [BASE_TO_COLUMN.get(b, b) for b in sequence]\n",
        "    one_hot = np.zeros((seq_len, 5))\n",
        "\n",
        "    # Assign the found positions to 1\n",
        "    one_hot[np.arange(seq_len), np.array(seq_in_index)] = 1\n",
        "\n",
        "    return one_hot\n",
        "\n",
        "def sequence_dictionary(filename):\n",
        "    \"\"\"\n",
        "\tCreates a dataframe that contains the sequence of each locus for each isolate\n",
        "\tNote that this function splits the identifier names in the fasta file on '/'\n",
        "\tand takes the last entry\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tfilename: str\n",
        "\t\tpath to directory containing genotype data (one fasta file containing\n",
        "\t\tsequences for all isolates at a particular locus)\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tpd.DataFrame with one column, indexed by strain name\n",
        "\t\tcolumn name will be the beginning string of the file name\n",
        "\t\"\"\"\n",
        "    seq_dict = SeqIO.to_dict(\n",
        "        SeqIO.parse(filename, \"fasta\"),\n",
        "        key_function=lambda x: x.id.split(\"/\")[-1].split(\".cut\")[0])\n",
        "\n",
        "    # create a dictionary of identifier: sequence\n",
        "    for identifier, sequence in seq_dict.items():\n",
        "        seq_dict[str(identifier)] = str(sequence.seq)\n",
        "\n",
        "    df = pd.DataFrame.from_dict(seq_dict, orient='index')\n",
        "    gene_name = filename.split(\"/\")[-1].split(\"_\")[0]\n",
        "    df.columns = [gene_name]\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def make_genotype_df(genotype_input_directory):\n",
        "    \"\"\"\n",
        "    Create a dataframe with the genotypes for each isolate at each locus\n",
        "    Hard-codes the ordering of the loci so that they are in same order upon re-runs\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tgenotype_input_directory: str\n",
        "\t\tpath to directory containing fasta files of genotype inputs\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tpd.DataFrame:\n",
        "\t\tindexed by isolate name, one column per locus\n",
        "\t\"\"\"\n",
        "    # Make a df that combines all genotype data\n",
        "    dfs_list = []\n",
        "\n",
        "    locus_order = [\n",
        "        \"acpM-kasA_20201206\",\n",
        "        \"gid_20201206\",\n",
        "        \"rpsA_20201206\",\n",
        "        \"clpC_20201213\",\n",
        "        \"embCAB_20201206\",\n",
        "        \"aftB-ubiA_20201206\",\n",
        "        \"rrs-rrl_20201206\",\n",
        "        \"ethAR_20201206\",\n",
        "        \"oxyR-ahpC_20201206\",\n",
        "        \"tlyA_20201206\",\n",
        "        \"KatG_20201206\",\n",
        "        \"rpsL_20201206\",\n",
        "        \"rpoBC_20201206\",\n",
        "        \"FabG1-inhA_20201206\",\n",
        "        \"eis_20201206\",\n",
        "        \"gyrBA_20201206\",\n",
        "        \"panD_20201213\",\n",
        "        \"pncA_20201206\"\n",
        "    ]\n",
        "\n",
        "    for l in locus_order:\n",
        "        df_file = f\"{genotype_input_directory}/{l}.fasta\"\n",
        "        print(\"reading fasta file\", df_file)\n",
        "        _df = sequence_dictionary(df_file)\n",
        "        print(\"found this many seqs\", len(_df))\n",
        "        dfs_list.append(_df)\n",
        "    df_genos = dfs_list[0].join(dfs_list[1:], how='outer')\n",
        "    print(\"size of df geno\", len(df_genos))\n",
        "    return df_genos\n",
        "\n",
        "\n",
        "# Change phenotype data to 0s and 1s\n",
        "def rs_encoding_to_numeric(df_geno_pheno, drugs_list):\n",
        "    \"\"\"\n",
        "\tCreates a matrix of y values (resistance/sensitivity)\n",
        "\tto each drug, encoded as 0's and 1's\n",
        "\n",
        "    Note: encodes sensitivity as 1, resistance as 0, missing as -1\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tdf_geno_pheno: pd.DataFrame\n",
        "        Dataframe containing resistance values to be converted from \"R\" and \"S\" to 0 and 1\n",
        "\n",
        "    drugs_list: list of str\n",
        "        list of columns in df_geno_pheno containing the drug resistance info to be encoded\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tpd.Dataframe\n",
        "        has same index as df_geno_pheno, columns corresponding to drugs_list\n",
        "        contains numeric encoding of resistance values\n",
        "\n",
        "    np.ndarray\n",
        "        corresponds to above pd.DataFrame.values\n",
        "\t\"\"\"\n",
        "\n",
        "    y_all_rs = df_geno_pheno[drugs_list]\n",
        "    y_all_rs = y_all_rs.fillna('-1')\n",
        "    y_all_rs = y_all_rs.astype(str)\n",
        "    resistance_categories = {'R': 0, 'S': 1, '-1.0': -1, '-1': -1}\n",
        "\n",
        "    y_all = y_all_rs.copy()\n",
        "    for key, val in resistance_categories.items():\n",
        "        y_all[y_all_rs == key] = val\n",
        "\n",
        "    y_all.index = list(range(0, y_all.shape[0]))\n",
        "\n",
        "    y_all_array = y_all.values\n",
        "\n",
        "    return y_all, y_all_array\n",
        "\n",
        "\n",
        "def alpha_mat(subset_y, df_geno_pheno, weight=1.):\n",
        "    \"\"\"\n",
        "\tcreates alpha matrix (reflects proportion of strains resistant\n",
        "\t(-ve)/sensitive (+ve)).\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tdf_geno_pheno: pd.DataFrame\n",
        "\t\tDataframe where last 23 columns contain R/S encoding\n",
        "\t\tof resistance vs sensitivity to antibiotics\n",
        "\n",
        "\tsubset_y: np.ndarray\n",
        "\t\tDataframe generated by the function rs_encoding_to_numeric\n",
        "\t\tcontaining matrix of resistance values (0 or 1) for each drug,\n",
        "\t\tthen subset for indexes (strains) with phenotype data available\n",
        "    weight: float\n",
        "        Weight by which to multiply the sensitive class (to up or downweight\n",
        "        sensitive relative to resistant strains)\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tnp.ndarray of weighted resistance/sensitivity values proportionate to no.\n",
        "\t\tof strains with phenotypic data.\n",
        "\t\"\"\"\n",
        "    # Drugs\n",
        "    drugs = ['RIFAMPICIN', 'ISONIAZID', 'PYRAZINAMIDE',\n",
        "             'ETHAMBUTOL', 'STREPTOMYCIN', 'LEVOFLOXACIN',\n",
        "             'CAPREOMYCIN', 'AMIKACIN', 'MOXIFLOXACIN',\n",
        "             'OFLOXACIN', 'KANAMYCIN', 'ETHIONAMIDE',\n",
        "             'CIPROFLOXACIN']\n",
        "\n",
        "    num_drugs = len(drugs)\n",
        "\n",
        "    y_cnn = subset_y\n",
        "\n",
        "    # generate alpha matrix\n",
        "    alphas = np.zeros(num_drugs, dtype=np.float)\n",
        "    alpha_matrix = np.zeros_like(y_cnn, dtype=np.float)\n",
        "\n",
        "    for drug in range(num_drugs):\n",
        "\n",
        "        resistant = len(np.squeeze(np.where(y_cnn[:, drug] == 0.)))\n",
        "        sensitive = len(np.squeeze(np.where(y_cnn[:, drug] == 1.)))\n",
        "        alphas[drug] = resistant / float(resistant + sensitive)\n",
        "        alpha_matrix[:, drug][np.where(y_cnn[:, drug] == 1.0)] = weight * alphas[drug]\n",
        "        alpha_matrix[:, drug][np.where(y_cnn[:, drug] == 0.0)] = - alphas[drug]\n",
        "\n",
        "    return alpha_matrix\n",
        "\n",
        "\n",
        "def make_geno_pheno_pkl(**kwargs):\n",
        "    \"\"\"\n",
        "    Creates and saves a pd.DataFrame as a pkl that contains the numeric encoded\n",
        "    phenotype information and the one-hot encoded sequence information for each isolate\n",
        "\n",
        "    Required kwargs:\n",
        "        phenotype_file: path to input phenotype file with columns \"Isolate\" and drug names\n",
        "        genotype_input_directory: path to directory with input fasta files\n",
        "        pkl_file: path to save the complete genotype/phenotype file\n",
        "\t\"\"\"\n",
        "\n",
        "    output_path = kwargs['output_path']\n",
        "\n",
        "    # get table for phenotypes\n",
        "    df_phenos = pd.read_csv(kwargs['phenotype_file'], index_col=\"Isolate\", sep=\",\", dtype=str).fillna(-1)\n",
        "\n",
        "    # make table of all genotypes\n",
        "    df_genos = make_genotype_df(kwargs['genotype_input_directory'])\n",
        "\n",
        "    # to save on RAM, only save genotypes for isolates for which we have phenotypes\n",
        "    isolate_ids = list(df_phenos.index)\n",
        "    n_strains = len(isolate_ids)\n",
        "    df_genos.index = df_genos.index.astype(str)\n",
        "    df_genos = df_genos.loc[df_genos.index.intersection(isolate_ids)]\n",
        "\n",
        "    # Drop rows where we're missing the sequence for a locus\n",
        "    df_genos = df_genos.dropna(axis=\"index\")\n",
        "\n",
        "    #\n",
        "    # # Apply one-hot encoding function to get each isolate sequence\n",
        "    print('making one hot encoding for...')\n",
        "    for column in df_genos.columns:\n",
        "        print(\"...\", column)\n",
        "        df_genos[column + \"_one_hot\"] = df_genos[column].apply(np.vectorize(get_one_hot))\n",
        "\n",
        "    # combined dataframe of all genotypes and phenotypes\n",
        "    df_geno_pheno_full = df_genos.join(df_phenos, how='inner')\n",
        "\n",
        "    pkl_file = kwargs[\"pkl_file\"]\n",
        "    df_geno_pheno_full.to_pickle(pkl_file)\n",
        "\n",
        "\n",
        "def create_X(df_geno_pheno):\n",
        "    \"\"\"\n",
        "\tCreate an input X matrix, with output dimensions:\n",
        "\t\tn_strains x 5 (one-hot) x longest locus length x no. of loci\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_geno_pheno: pd.DataFrame\n",
        "        generated by make_geno_pheno_pkl, contains the numeric encoded\n",
        "        phenotype information and the one-hot encoded sequence information for each isolate\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        with shape N_strains, 5, L_longest_locus, N_loci\n",
        "        contains the one-hot encoded sequence information for each locus for each strain\n",
        "\t\"\"\"\n",
        "\n",
        "    def _get_shapes(df_geno_pheno):\n",
        "        \"\"\"\n",
        "\t\tFinds the length of each gene in the input dataframe\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tdf_geno_pheno: pd.Dataframe\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tdict of str: int\n",
        "\t\t\tlength of coordinates in each column\n",
        "\t\t\"\"\"\n",
        "        shapes = {}\n",
        "        for column in df_geno_pheno.columns:\n",
        "            if \"one_hot\" in column:\n",
        "                shapes[column] = df_geno_pheno.loc[df_geno_pheno.index[0], column].shape[0]\n",
        "\n",
        "        return shapes\n",
        "\n",
        "    shapes = _get_shapes(df_geno_pheno)\n",
        "\n",
        "    # Length of longest gene locus\n",
        "    n_genes = len(shapes)\n",
        "    L_longest = max(list(shapes.values()))\n",
        "    print(\"found n genes\", n_genes, \"and longest gene\", L_longest)\n",
        "\n",
        "    # Number of strains in model\n",
        "    n_strains = df_geno_pheno.shape[0]\n",
        "\n",
        "    # define shape of matrix - fill with zeros (effectively accomplishes padding)\n",
        "    X = np.zeros((n_strains, 5, L_longest, n_genes))\n",
        "\n",
        "    # for each strain and gene locus\n",
        "    for idx, strain in enumerate(df_geno_pheno.index):\n",
        "        for gene_index, gene in enumerate(shapes.keys()):\n",
        "            one_hot_gene = df_geno_pheno.loc[strain, gene]\n",
        "            X[idx, :, range(0, one_hot_gene.shape[0]), gene_index] = one_hot_gene\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "def masked_multi_weighted_bce(alpha, y_pred):\n",
        "    \"\"\"\n",
        "\tCalculates the masked weighted binary cross-entropy in multi-classification\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\talpha: an element from the alpha matrix, a matrix of target y values weighted\n",
        "\t\tby proportion of strains with resistance data for any given drug\n",
        "\ty_pred: model-predicted y values\n",
        "    weights: list of float (optional, default=[1., 1.])\n",
        "        A list of two weights to be applied to the sensitive and resistant n_strains,\n",
        "        respectively\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tscalar value of the masked weighted BCE.\n",
        "\t\"\"\"\n",
        "    y_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())\n",
        "    y_true_ = K.cast(K.greater(alpha, 0.), K.floatx())\n",
        "    mask = K.cast(K.not_equal(alpha, 0.), K.floatx())\n",
        "    num_not_missing = K.sum(mask, axis=-1)\n",
        "    alpha = K.abs(alpha)\n",
        "    bce = - alpha * y_true_ * K.log(y_pred) - (1.0 - alpha) * (1.0 - y_true_) * K.log(1.0 - y_pred)\n",
        "    masked_bce = bce * mask\n",
        "    return K.sum(masked_bce, axis=-1) / num_not_missing\n",
        "\n",
        "\n",
        "def masked_weighted_accuracy(alpha, y_pred):\n",
        "    \"\"\"\n",
        "\tCalculates the mased weighted accuracy of a model's predictions\n",
        "\tParameters\n",
        "\t----------\n",
        "\talpha: an element from the alpha matrix, a matrix of target y values weighted\n",
        "\t\tby proportion of strains with resistance data for any given drug\n",
        "\ty_pred: model-predicted y values\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tscalar value of the masked weighted accuracy.\n",
        "\t\"\"\"\n",
        "\n",
        "    total = K.sum(K.cast(K.not_equal(alpha, 0.), K.floatx()))\n",
        "    y_true_ = K.cast(K.greater(alpha, 0.), K.floatx())\n",
        "    mask = K.cast(K.not_equal(alpha, 0.), K.floatx())\n",
        "    correct = K.sum(K.cast(K.equal(y_true_, K.round(y_pred)), K.floatx()) * mask)\n",
        "    return correct / total\n",
        "\n",
        "def load_alpha_matrix(alpha_matrix_path, y_array, df_geno_pheno, **kwargs):\n",
        "    \"\"\"\n",
        "    Loads in the alpha matrix, if file exists, otherwise creates alpha matrix\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha_matrix_path: str\n",
        "        path to alpha matrix. Will be used to save matrix if matrix does not exist\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        The alpha matrix\n",
        "    \"\"\"\n",
        "\n",
        "    if os.path.isfile(alpha_matrix_path):\n",
        "        print(\"alpha matrix already exists, loading alpha matrix\")\n",
        "        alpha_matrix = alpha_matrix = np.loadtxt(alpha_matrix_path, delimiter=',')\n",
        "    else:\n",
        "        print(\"creating alpha matrix\")\n",
        "        if \"weight_of_sensitive_class\" in kwargs:\n",
        "            print('creating alpha matrix with weight', kwargs[\"weight_of_sensitive_class\"])\n",
        "            alpha_matrix = alpha_mat(y_array, df_geno_pheno, kwargs[\"weight_of_sensitive_class\"])\n",
        "        else:\n",
        "            alpha_matrix = alpha_mat(y_array, df_geno_pheno)\n",
        "        np.savetxt(alpha_matrix_path, alpha_matrix, delimiter=',')\n",
        "\n",
        "    print(\"the shape of the alpha_matrix: {}\".format(alpha_matrix.shape))\n",
        "    return alpha_matrix\n",
        "\n",
        "def split_into_traintest(X_sparse, df_geno_pheno, category):\n",
        "    \"\"\"\n",
        "    Splits the X dataframe into training and test set based on annotation in df_geno_pheno\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_sparse: sparse.COO\n",
        "        a sparse-encoded np.ndarray containing genetic information for all isolates\n",
        "    df_geno_pheno: pd.DataFrame\n",
        "        Dataframe of genetic and phenotypic information. Contains the exact isolates in the exact order used to create X_sparse.\n",
        "        Contains a column called \"category\" that will be used to split isolates into training and test\n",
        "    category: str\n",
        "        Name of the training set category\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    sparse.COO\n",
        "        a sparse-encoded np.ndarray containing genetic information for all TRAINING SET isolates\n",
        "    sparse.COO\n",
        "        a sparse-encoded np.ndarray containing genetic information for all TEST SET isolates\n",
        "    \"\"\"\n",
        "    X_all = X_sparse.todense()\n",
        "\n",
        "    df_geno_pheno = df_geno_pheno.reset_index(drop=True)\n",
        "\n",
        "    train_indices = df_geno_pheno.query(\"category==@category\").index\n",
        "    test_indices = df_geno_pheno.query(\"category!=@category\").index\n",
        "\n",
        "    print(\"splitting X pkl\")\n",
        "    X_train = X_all[train_indices, :]\n",
        "    X_test = X_all[test_indices, :]\n",
        "    del X_all\n",
        "\n",
        "    X_sparse_train = sparse.COO(X_train)\n",
        "    sparse.save_npz(pkl_file_sparse_train, X_sparse_train, compressed=False)\n",
        "\n",
        "    X_sparse_test = sparse.COO(X_test)\n",
        "    sparse.save_npz(pkl_file_sparse_test, X_sparse_test, compressed=False)\n",
        "\n",
        "    return X_sparse_train, X_sparse_test\n",
        "\n",
        "\n",
        "def get_threshold_val(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute the optimal threshold for prediction  based on the max sum of specificity and Sensitivity\n",
        "\n",
        "    NB that we encoded R as 0, S as 1, so smaller predictions indicate higher chance of resistance\n",
        "\n",
        "    We count falsely predicted resistance as a false positive, falsely predicted sensitivity as a false negative\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true: np.array\n",
        "        Actual labels for isolates\n",
        "    y_pred: np.array\n",
        "        Predicted labels for isolates\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict of str -> float with entries:\n",
        "        sens: sensitivity at chosen threshold\n",
        "        spec: specificity at chosen threshold\n",
        "        threshold: chosen threshold value\n",
        "    \"\"\"\n",
        "\n",
        "    num_samples = y_pred.shape[0]\n",
        "    fpr_ = []\n",
        "    tpr_ = []\n",
        "    thresholds = np.linspace(0, 1, 101)\n",
        "    num_sensitive = np.sum(y_true==1)\n",
        "    num_resistant = np.sum(y_true==0)\n",
        "    for threshold in thresholds:\n",
        "\n",
        "        fp_ = 0 # number of false positives\n",
        "        tp_ = 0 # number of true positives\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            # If y is predicted resistant\n",
        "            if (y_pred[i] < threshold):\n",
        "                if (y_true[i] == 1): fp_ += 1\n",
        "                if (y_true[i] == 0): tp_ += 1\n",
        "\n",
        "        fpr_.append(fp_ / float(num_sensitive))\n",
        "        tpr_.append(tp_ / float(num_resistant))\n",
        "\n",
        "    fpr_ = np.array(fpr_)\n",
        "    tpr_ = np.array(tpr_)\n",
        "\n",
        "    # valid_inds = np.where(fpr_ <= 0.1)\n",
        "    valid_inds = np.arange(101)\n",
        "    sens_spec_sum = (1 - fpr_) + tpr_\n",
        "    best_sens_spec_sum = np.max(sens_spec_sum[valid_inds])\n",
        "    best_inds = np.where(best_sens_spec_sum == sens_spec_sum[valid_inds])\n",
        "\n",
        "    if best_inds[0].shape[0] == 1:\n",
        "        best_sens_spec_ind = np.array(np.squeeze(best_inds))\n",
        "    else:\n",
        "        best_sens_spec_ind = np.array(np.squeeze(best_inds))[-1]\n",
        "\n",
        "    return {'threshold': np.squeeze(thresholds[valid_inds][best_sens_spec_ind]),\n",
        "            'spec': 1 - fpr_[valid_inds][best_sens_spec_ind],\n",
        "            'sens': tpr_[valid_inds][best_sens_spec_ind]}\n"
      ],
      "metadata": {
        "id": "t1u8p_ca-4O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Functions for running CNN on MTB data to predict ABR phenotypes using tensorflow 1\n",
        "Authors:\n",
        "\tMichael Chen (original version)\n",
        "\tAnna G. Green\n",
        "\tChang Ho Yoon\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import sys\n",
        "import glob\n",
        "import os\n",
        "import yaml\n",
        "import sparse\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "from Bio import SeqIO\n",
        "\n",
        "# Mapping to use for one-hot encoding\n",
        "BASE_TO_COLUMN = {'A': 0, 'C': 1, 'T': 2, 'G': 3, '-': 4}\n",
        "\n",
        "# Get one hot vector\n",
        "def get_one_hot(sequence):\n",
        "    \"\"\"\n",
        "\tCreates a one-hot encoding of a sequence\n",
        "\tParameters\n",
        "\t----------\n",
        "\tsequence: iterable of str\n",
        "\t\tSequence containing only ACTG- characters\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tnp.ndarray of int\n",
        "\t\tL (seq len) x 5 one-hot encoded sequence\n",
        "\t\"\"\"\n",
        "\n",
        "    seq_len = len(sequence)\n",
        "    seq_in_index = [BASE_TO_COLUMN.get(b, b) for b in sequence]\n",
        "    one_hot = np.zeros((seq_len, 5))\n",
        "\n",
        "    # Assign the found positions to 1\n",
        "    one_hot[np.arange(seq_len), np.array(seq_in_index)] = 1\n",
        "\n",
        "    return one_hot\n",
        "\n",
        "def sequence_dictionary(filename):\n",
        "    \"\"\"\n",
        "\tCreates a dataframe that contains the sequence of each locus for each isolate\n",
        "\tNote that this function splits the identifier names in the fasta file on '/'\n",
        "\tand takes the last entry\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tfilename: str\n",
        "\t\tpath to directory containing genotype data (one fasta file containing\n",
        "\t\tsequences for all isolates at a particular locus)\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tpd.DataFrame with one column, indexed by strain name\n",
        "\t\tcolumn name will be the beginning string of the file name\n",
        "\t\"\"\"\n",
        "    seq_dict = SeqIO.to_dict(\n",
        "        SeqIO.parse(filename, \"fasta\"),\n",
        "        key_function=lambda x: x.id.split(\"/\")[-1].split(\".cut\")[0])\n",
        "\n",
        "    # create a dictionary of identifier: sequence\n",
        "    for identifier, sequence in seq_dict.items():\n",
        "        seq_dict[str(identifier)] = str(sequence.seq)\n",
        "\n",
        "    df = pd.DataFrame.from_dict(seq_dict, orient='index')\n",
        "    gene_name = filename.split(\"/\")[-1].split(\"_\")[0]\n",
        "    df.columns = [gene_name]\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def make_genotype_df(genotype_input_directory):\n",
        "    \"\"\"\n",
        "    Create a dataframe with the genotypes for each isolate at each locus\n",
        "    Hard-codes the ordering of the loci so that they are in same order upon re-runs\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tgenotype_input_directory: str\n",
        "\t\tpath to directory containing fasta files of genotype inputs\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tpd.DataFrame:\n",
        "\t\tindexed by isolate name, one column per locus\n",
        "\t\"\"\"\n",
        "    # Make a df that combines all genotype data\n",
        "    dfs_list = []\n",
        "\n",
        "    locus_order = [\n",
        "        \"acpM-kasA_20201206\",\n",
        "        \"gid_20201206\",\n",
        "        \"rpsA_20201206\",\n",
        "        \"clpC_20201213\",\n",
        "        \"embCAB_20201206\",\n",
        "        \"aftB-ubiA_20201206\",\n",
        "        \"rrs-rrl_20201206\",\n",
        "        \"ethAR_20201206\",\n",
        "        \"oxyR-ahpC_20201206\",\n",
        "        \"tlyA_20201206\",\n",
        "        \"KatG_20201206\",\n",
        "        \"rpsL_20201206\",\n",
        "        \"rpoBC_20201206\",\n",
        "        \"FabG1-inhA_20201206\",\n",
        "        \"eis_20201206\",\n",
        "        \"gyrBA_20201206\",\n",
        "        \"panD_20201213\",\n",
        "        \"pncA_20201206\"\n",
        "    ]\n",
        "\n",
        "    for l in locus_order:\n",
        "        df_file = f\"{genotype_input_directory}/{l}.fasta\"\n",
        "        print(\"reading fasta file\", df_file)\n",
        "        _df = sequence_dictionary(df_file)\n",
        "        print(\"found this many seqs\", len(_df))\n",
        "        dfs_list.append(_df)\n",
        "    df_genos = dfs_list[0].join(dfs_list[1:], how='outer')\n",
        "    print(\"size of df geno\", len(df_genos))\n",
        "    return df_genos\n",
        "\n",
        "\n",
        "# Change phenotype data to 0s and 1s\n",
        "def rs_encoding_to_numeric(df_geno_pheno, drugs_list):\n",
        "    \"\"\"\n",
        "\tCreates a matrix of y values (resistance/sensitivity)\n",
        "\tto each drug, encoded as 0's and 1's\n",
        "\n",
        "    Note: encodes sensitivity as 1, resistance as 0, missing as -1\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tdf_geno_pheno: pd.DataFrame\n",
        "        Dataframe containing resistance values to be converted from \"R\" and \"S\" to 0 and 1\n",
        "\n",
        "    drugs_list: list of str\n",
        "        list of columns in df_geno_pheno containing the drug resistance info to be encoded\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tpd.Dataframe\n",
        "        has same index as df_geno_pheno, columns corresponding to drugs_list\n",
        "        contains numeric encoding of resistance values\n",
        "\n",
        "    np.ndarray\n",
        "        corresponds to above pd.DataFrame.values\n",
        "\t\"\"\"\n",
        "\n",
        "    y_all_rs = df_geno_pheno[drugs_list]\n",
        "    y_all_rs = y_all_rs.fillna('-1')\n",
        "    y_all_rs = y_all_rs.astype(str)\n",
        "    resistance_categories = {'R': 0, 'S': 1, '-1.0': -1, '-1': -1}\n",
        "\n",
        "    y_all = y_all_rs.copy()\n",
        "    for key, val in resistance_categories.items():\n",
        "        y_all[y_all_rs == key] = val\n",
        "\n",
        "    y_all.index = list(range(0, y_all.shape[0]))\n",
        "\n",
        "    y_all_array = y_all.values\n",
        "\n",
        "    return y_all, y_all_array\n",
        "\n",
        "\n",
        "def alpha_mat(subset_y, df_geno_pheno, weight=1.):\n",
        "    \"\"\"\n",
        "\tcreates alpha matrix (reflects proportion of strains resistant\n",
        "\t(-ve)/sensitive (+ve)).\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tdf_geno_pheno: pd.DataFrame\n",
        "\t\tDataframe where last 23 columns contain R/S encoding\n",
        "\t\tof resistance vs sensitivity to antibiotics\n",
        "\n",
        "\tsubset_y: np.ndarray\n",
        "\t\tDataframe generated by the function rs_encoding_to_numeric\n",
        "\t\tcontaining matrix of resistance values (0 or 1) for each drug,\n",
        "\t\tthen subset for indexes (strains) with phenotype data available\n",
        "    weight: float\n",
        "        Weight by which to multiply the sensitive class (to up or downweight\n",
        "        sensitive relative to resistant strains)\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tnp.ndarray of weighted resistance/sensitivity values proportionate to no.\n",
        "\t\tof strains with phenotypic data.\n",
        "\t\"\"\"\n",
        "    # Drugs\n",
        "    drugs = ['RIFAMPICIN', 'ISONIAZID', 'PYRAZINAMIDE',\n",
        "             'ETHAMBUTOL', 'STREPTOMYCIN', 'LEVOFLOXACIN',\n",
        "             'CAPREOMYCIN', 'AMIKACIN', 'MOXIFLOXACIN',\n",
        "             'OFLOXACIN', 'KANAMYCIN', 'ETHIONAMIDE',\n",
        "             'CIPROFLOXACIN']\n",
        "\n",
        "    num_drugs = len(drugs)\n",
        "\n",
        "    y_cnn = subset_y\n",
        "\n",
        "    # generate alpha matrix\n",
        "    alphas = np.zeros(num_drugs, dtype=np.float)\n",
        "    alpha_matrix = np.zeros_like(y_cnn, dtype=np.float)\n",
        "\n",
        "    for drug in range(num_drugs):\n",
        "\n",
        "        resistant = len(np.squeeze(np.where(y_cnn[:, drug] == 0.)))\n",
        "        sensitive = len(np.squeeze(np.where(y_cnn[:, drug] == 1.)))\n",
        "        alphas[drug] = resistant / float(resistant + sensitive)\n",
        "        alpha_matrix[:, drug][np.where(y_cnn[:, drug] == 1.0)] = weight * alphas[drug]\n",
        "        alpha_matrix[:, drug][np.where(y_cnn[:, drug] == 0.0)] = - alphas[drug]\n",
        "\n",
        "    return alpha_matrix\n",
        "\n",
        "\n",
        "def make_geno_pheno_pkl(**kwargs):\n",
        "    \"\"\"\n",
        "    Creates and saves a pd.DataFrame as a pkl that contains the numeric encoded\n",
        "    phenotype information and the one-hot encoded sequence information for each isolate\n",
        "\n",
        "    Required kwargs:\n",
        "        phenotype_file: path to input phenotype file with columns \"Isolate\" and drug names\n",
        "        genotype_input_directory: path to directory with input fasta files\n",
        "        pkl_file: path to save the complete genotype/phenotype file\n",
        "\t\"\"\"\n",
        "\n",
        "    output_path = kwargs['output_path']\n",
        "\n",
        "    # get table for phenotypes\n",
        "    df_phenos = pd.read_csv(kwargs['phenotype_file'], index_col=\"Isolate\", sep=\",\", dtype=str).fillna(-1)\n",
        "\n",
        "    # make table of all genotypes\n",
        "    df_genos = make_genotype_df(kwargs['genotype_input_directory'])\n",
        "\n",
        "    # to save on RAM, only save genotypes for isolates for which we have phenotypes\n",
        "    isolate_ids = list(df_phenos.index)\n",
        "    n_strains = len(isolate_ids)\n",
        "    df_genos.index = df_genos.index.astype(str)\n",
        "    df_genos = df_genos.loc[df_genos.index.intersection(isolate_ids)]\n",
        "\n",
        "    # Drop rows where we're missing the sequence for a locus\n",
        "    df_genos = df_genos.dropna(axis=\"index\")\n",
        "\n",
        "    #\n",
        "    # # Apply one-hot encoding function to get each isolate sequence\n",
        "    print('making one hot encoding for...')\n",
        "    for column in df_genos.columns:\n",
        "        print(\"...\", column)\n",
        "        df_genos[column + \"_one_hot\"] = df_genos[column].apply(np.vectorize(get_one_hot))\n",
        "\n",
        "    # combined dataframe of all genotypes and phenotypes\n",
        "    df_geno_pheno_full = df_genos.join(df_phenos, how='inner')\n",
        "\n",
        "    pkl_file = kwargs[\"pkl_file\"]\n",
        "    df_geno_pheno_full.to_pickle(pkl_file)\n",
        "\n",
        "\n",
        "def create_X(df_geno_pheno):\n",
        "    \"\"\"\n",
        "\tCreate an input X matrix, with output dimensions:\n",
        "\t\tn_strains x 5 (one-hot) x longest locus length x no. of loci\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_geno_pheno: pd.DataFrame\n",
        "        generated by make_geno_pheno_pkl, contains the numeric encoded\n",
        "        phenotype information and the one-hot encoded sequence information for each isolate\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        with shape N_strains, 5, L_longest_locus, N_loci\n",
        "        contains the one-hot encoded sequence information for each locus for each strain\n",
        "\t\"\"\"\n",
        "\n",
        "    def _get_shapes(df_geno_pheno):\n",
        "        \"\"\"\n",
        "\t\tFinds the length of each gene in the input dataframe\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tdf_geno_pheno: pd.Dataframe\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tdict of str: int\n",
        "\t\t\tlength of coordinates in each column\n",
        "\t\t\"\"\"\n",
        "        shapes = {}\n",
        "        for column in df_geno_pheno.columns:\n",
        "            if \"one_hot\" in column:\n",
        "                shapes[column] = df_geno_pheno.loc[df_geno_pheno.index[0], column].shape[0]\n",
        "\n",
        "        return shapes\n",
        "\n",
        "    shapes = _get_shapes(df_geno_pheno)\n",
        "\n",
        "    # Length of longest gene locus\n",
        "    n_genes = len(shapes)\n",
        "    L_longest = max(list(shapes.values()))\n",
        "    print(\"found n genes\", n_genes, \"and longest gene\", L_longest)\n",
        "\n",
        "    # Number of strains in model\n",
        "    n_strains = df_geno_pheno.shape[0]\n",
        "\n",
        "    # define shape of matrix - fill with zeros (effectively accomplishes padding)\n",
        "    X = np.zeros((n_strains, 5, L_longest, n_genes))\n",
        "\n",
        "    # for each strain and gene locus\n",
        "    for idx, strain in enumerate(df_geno_pheno.index):\n",
        "        for gene_index, gene in enumerate(shapes.keys()):\n",
        "            one_hot_gene = df_geno_pheno.loc[strain, gene]\n",
        "            X[idx, :, range(0, one_hot_gene.shape[0]), gene_index] = one_hot_gene\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "def masked_multi_weighted_bce(alpha, y_pred):\n",
        "    \"\"\"\n",
        "\tCalculates the masked weighted binary cross-entropy in multi-classification\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\talpha: an element from the alpha matrix, a matrix of target y values weighted\n",
        "\t\tby proportion of strains with resistance data for any given drug\n",
        "\ty_pred: model-predicted y values\n",
        "    weights: list of float (optional, default=[1., 1.])\n",
        "        A list of two weights to be applied to the sensitive and resistant n_strains,\n",
        "        respectively\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tscalar value of the masked weighted BCE.\n",
        "\t\"\"\"\n",
        "    y_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())\n",
        "    y_true_ = K.cast(K.greater(alpha, 0.), K.floatx())\n",
        "    mask = K.cast(K.not_equal(alpha, 0.), K.floatx())\n",
        "    num_not_missing = K.sum(mask, axis=-1)\n",
        "    alpha = K.abs(alpha)\n",
        "    bce = - alpha * y_true_ * K.log(y_pred) - (1.0 - alpha) * (1.0 - y_true_) * K.log(1.0 - y_pred)\n",
        "    masked_bce = bce * mask\n",
        "    return K.sum(masked_bce, axis=-1) / num_not_missing\n",
        "\n",
        "\n",
        "def masked_weighted_accuracy(alpha, y_pred):\n",
        "    \"\"\"\n",
        "\tCalculates the mased weighted accuracy of a model's predictions\n",
        "\tParameters\n",
        "\t----------\n",
        "\talpha: an element from the alpha matrix, a matrix of target y values weighted\n",
        "\t\tby proportion of strains with resistance data for any given drug\n",
        "\ty_pred: model-predicted y values\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tscalar value of the masked weighted accuracy.\n",
        "\t\"\"\"\n",
        "\n",
        "    total = K.sum(K.cast(K.not_equal(alpha, 0.), K.floatx()))\n",
        "    y_true_ = K.cast(K.greater(alpha, 0.), K.floatx())\n",
        "    mask = K.cast(K.not_equal(alpha, 0.), K.floatx())\n",
        "    correct = K.sum(K.cast(K.equal(y_true_, K.round(y_pred)), K.floatx()) * mask)\n",
        "    return correct / total\n",
        "\n",
        "def load_alpha_matrix(alpha_matrix_path, y_array, df_geno_pheno, **kwargs):\n",
        "    \"\"\"\n",
        "    Loads in the alpha matrix, if file exists, otherwise creates alpha matrix\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha_matrix_path: str\n",
        "        path to alpha matrix. Will be used to save matrix if matrix does not exist\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        The alpha matrix\n",
        "    \"\"\"\n",
        "\n",
        "    if os.path.isfile(alpha_matrix_path):\n",
        "        print(\"alpha matrix already exists, loading alpha matrix\")\n",
        "        alpha_matrix = alpha_matrix = np.loadtxt(alpha_matrix_path, delimiter=',')\n",
        "    else:\n",
        "        print(\"creating alpha matrix\")\n",
        "        if \"weight_of_sensitive_class\" in kwargs:\n",
        "            print('creating alpha matrix with weight', kwargs[\"weight_of_sensitive_class\"])\n",
        "            alpha_matrix = alpha_mat(y_array, df_geno_pheno, kwargs[\"weight_of_sensitive_class\"])\n",
        "        else:\n",
        "            alpha_matrix = alpha_mat(y_array, df_geno_pheno)\n",
        "        np.savetxt(alpha_matrix_path, alpha_matrix, delimiter=',')\n",
        "\n",
        "    print(\"the shape of the alpha_matrix: {}\".format(alpha_matrix.shape))\n",
        "    return alpha_matrix\n",
        "\n",
        "def split_into_traintest(X_sparse, df_geno_pheno, category):\n",
        "    \"\"\"\n",
        "    Splits the X dataframe into training and test set based on annotation in df_geno_pheno\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_sparse: sparse.COO\n",
        "        a sparse-encoded np.ndarray containing genetic information for all isolates\n",
        "    df_geno_pheno: pd.DataFrame\n",
        "        Dataframe of genetic and phenotypic information. Contains the exact isolates in the exact order used to create X_sparse.\n",
        "        Contains a column called \"category\" that will be used to split isolates into training and test\n",
        "    category: str\n",
        "        Name of the training set category\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    sparse.COO\n",
        "        a sparse-encoded np.ndarray containing genetic information for all TRAINING SET isolates\n",
        "    sparse.COO\n",
        "        a sparse-encoded np.ndarray containing genetic information for all TEST SET isolates\n",
        "    \"\"\"\n",
        "    X_all = X_sparse.todense()\n",
        "\n",
        "    df_geno_pheno = df_geno_pheno.reset_index(drop=True)\n",
        "\n",
        "    train_indices = df_geno_pheno.query(\"category==@category\").index\n",
        "    test_indices = df_geno_pheno.query(\"category!=@category\").index\n",
        "\n",
        "    print(\"splitting X pkl\")\n",
        "    X_train = X_all[train_indices, :]\n",
        "    X_test = X_all[test_indices, :]\n",
        "    del X_all\n",
        "\n",
        "    X_sparse_train = sparse.COO(X_train)\n",
        "    sparse.save_npz(pkl_file_sparse_train, X_sparse_train, compressed=False)\n",
        "\n",
        "    X_sparse_test = sparse.COO(X_test)\n",
        "    sparse.save_npz(pkl_file_sparse_test, X_sparse_test, compressed=False)\n",
        "\n",
        "    return X_sparse_train, X_sparse_test\n",
        "\n",
        "\n",
        "def get_threshold_val(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute the optimal threshold for prediction  based on the max sum of specificity and Sensitivity\n",
        "\n",
        "    NB that we encoded R as 0, S as 1, so smaller predictions indicate higher chance of resistance\n",
        "\n",
        "    We count falsely predicted resistance as a false positive, falsely predicted sensitivity as a false negative\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true: np.array\n",
        "        Actual labels for isolates\n",
        "    y_pred: np.array\n",
        "        Predicted labels for isolates\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict of str -> float with entries:\n",
        "        sens: sensitivity at chosen threshold\n",
        "        spec: specificity at chosen threshold\n",
        "        threshold: chosen threshold value\n",
        "    \"\"\"\n",
        "\n",
        "    num_samples = y_pred.shape[0]\n",
        "    fpr_ = []\n",
        "    tpr_ = []\n",
        "    thresholds = np.linspace(0, 1, 101)\n",
        "    num_sensitive = np.sum(y_true==1)\n",
        "    num_resistant = np.sum(y_true==0)\n",
        "    for threshold in thresholds:\n",
        "\n",
        "        fp_ = 0 # number of false positives\n",
        "        tp_ = 0 # number of true positives\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            # If y is predicted resistant\n",
        "            if (y_pred[i] < threshold):\n",
        "                if (y_true[i] == 1): fp_ += 1\n",
        "                if (y_true[i] == 0): tp_ += 1\n",
        "\n",
        "        fpr_.append(fp_ / float(num_sensitive))\n",
        "        tpr_.append(tp_ / float(num_resistant))\n",
        "\n",
        "    fpr_ = np.array(fpr_)\n",
        "    tpr_ = np.array(tpr_)\n",
        "\n",
        "    # valid_inds = np.where(fpr_ <= 0.1)\n",
        "    valid_inds = np.arange(101)\n",
        "    sens_spec_sum = (1 - fpr_) + tpr_\n",
        "    best_sens_spec_sum = np.max(sens_spec_sum[valid_inds])\n",
        "    best_inds = np.where(best_sens_spec_sum == sens_spec_sum[valid_inds])\n",
        "\n",
        "    if best_inds[0].shape[0] == 1:\n",
        "        best_sens_spec_ind = np.array(np.squeeze(best_inds))\n",
        "    else:\n",
        "        best_sens_spec_ind = np.array(np.squeeze(best_inds))[-1]\n",
        "\n",
        "    return {'threshold': np.squeeze(thresholds[valid_inds][best_sens_spec_ind]),\n",
        "            'spec': 1 - fpr_[valid_inds][best_sens_spec_ind],\n",
        "            'sens': tpr_[valid_inds][best_sens_spec_ind]}\n"
      ],
      "metadata": {
        "id": "-xfJ7GNP_JTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Code for creating the input X data point for the reference genome, MTB_H37Rv\n",
        "\n",
        "Authors:\n",
        "Anna G. Green\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import glob\n",
        "import os\n",
        "import yaml\n",
        "import sparse\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tb_cnn_codebase import *\n",
        "\n",
        "geno_df = make_genotype_df(\"fasta_files/\")\n",
        "df_genos = geno_df.query('index ==\"MT_H37Rv\"')\n",
        "\n",
        "print('making one hot encoding for...')\n",
        "for column in df_genos.columns:\n",
        "    print(\"...\", column)\n",
        "    df_genos[column + \"_one_hot\"] = df_genos[column].apply(np.vectorize(get_one_hot))\n",
        "\n",
        "X = create_X(df_genos)\n",
        "\n",
        "print(X.shape)\n",
        "\n",
        "np.save( \"h37rv_input.npy\", X)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "pXVRu_Li_OCv",
        "outputId": "62616e57-2d83-4023-a6f3-41447f7e519c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-a9eb6b884c0b>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtb_cnn_codebase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mgeno_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_genotype_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fasta_files/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tb_cnn_codebase'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Bio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh7nxGYE9pp4",
        "outputId": "9b4b1071-3610-4c7b-e839-a2ee09be7fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Bio\n",
            "  Downloading bio-1.5.9-py3-none-any.whl (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.4/276.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting biopython>=1.80 (from Bio)\n",
            "  Downloading biopython-1.81-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Bio) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from Bio) (4.66.1)\n",
            "Collecting mygene (from Bio)\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from Bio) (1.5.3)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from Bio) (1.7.0)\n",
            "Collecting gprofiler-official (from Bio)\n",
            "  Downloading gprofiler_official-1.0.0-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython>=1.80->Bio) (1.23.5)\n",
            "Collecting biothings-client>=0.2.6 (from mygene->Bio)\n",
            "  Downloading biothings_client-0.3.0-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->Bio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->Bio) (2023.3.post1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->Bio) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch->Bio) (23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (2023.7.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->Bio) (1.16.0)\n",
            "Installing collected packages: biopython, gprofiler-official, biothings-client, mygene, Bio\n",
            "Successfully installed Bio-1.5.9 biopython-1.81 biothings-client-0.3.0 gprofiler-official-1.0.0 mygene-3.2.2\n"
          ]
        }
      ]
    }
  ]
}